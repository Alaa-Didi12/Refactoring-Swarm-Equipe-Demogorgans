# src/llm/gemini_client.py - VERSION CORRECTE
import os
import json
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

def get_working_model():
    """Trouve un modÃ¨le qui fonctionne."""
    api_key = os.getenv("GOOGLE_API_KEY", "").strip().strip('"').strip("'")
    
    if not api_key or not api_key.startswith("AIza"):
        return None
    
    # ModÃ¨les Ã  essayer (les plus courants)
    models_to_try = [
        'gemini-1.5-pro-latest',  # Nouveau nom
        'gemini-1.5-pro',         # Ancien nom
        'gemini-1.0-pro-latest',  # Alternative
        'gemini-1.0-pro',
        'gemini-pro',             # Le plus basique
        'models/gemini-1.5-pro-001',  # Format complet
        'models/gemini-1.0-pro-001',
    ]
    
    genai.configure(api_key=api_key)
    
    for model_name in models_to_try:
        try:
            print(f"   ğŸ” Test du modÃ¨le: {model_name}")
            model = genai.GenerativeModel(model_name)
            # Test rapide
            response = model.generate_content("test", generation_config={"max_output_tokens": 1})
            print(f"   âœ… ModÃ¨le fonctionnel: {model_name}")
            return model_name
        except Exception as e:
            continue
    
    return None

# Cache le modÃ¨le fonctionnel
WORKING_MODEL = get_working_model()

def call_gemini(system_prompt: str, user_prompt: str) -> str:
    """Utilise le VRAI Gemini avec le bon modÃ¨le."""
    
    api_key = os.getenv("GOOGLE_API_KEY", "").strip().strip('"').strip("'")
    
    if not api_key or not api_key.startswith("AIza"):
        print("   âŒ ClÃ© API invalide")
        return _fallback_response(system_prompt, user_prompt)
    
    if not WORKING_MODEL:
        print("   âŒ Aucun modÃ¨le Gemini fonctionnel trouvÃ©")
        return _fallback_response(system_prompt, user_prompt)
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(WORKING_MODEL)
        
        print(f"   ğŸ¤– Appel Gemini avec {WORKING_MODEL}")
        
        # PrÃ©pare le prompt
        full_prompt = f"""{system_prompt}

{user_prompt}

RÃ©ponds en franÃ§ais."""
        
        # Appel API
        response = model.generate_content(
            full_prompt,
            generation_config={
                "temperature": 0.1,
                "max_output_tokens": 2000,
            }
        )
        
        print(f"   âœ… RÃ©ponse Gemini reÃ§ue ({len(response.text)} caractÃ¨res)")
        return response.text
        
    except Exception as e:
        print(f"   âŒ Erreur Gemini: {e}")
        return _fallback_response(system_prompt, user_prompt)

def _fallback_response(system_prompt: str, user_prompt: str) -> str:
    """Fallback si Gemini Ã©choue."""
    print("   ğŸ”„ Fallback simulation")
    
    # Simulation basique
    return json.dumps({
        "message": "Gemini API non disponible - mode simulation",
        "test": "ok"
    })

# Test au dÃ©marrage
if __name__ == "__main__":
    print("ğŸ§ª Test Gemini...")
    if WORKING_MODEL:
        print(f"âœ… ModÃ¨le trouvÃ©: {WORKING_MODEL}")
        # Test rÃ©el
        test = call_gemini("Tu es un test", "Dis bonjour")
        print(f"RÃ©ponse: {test[:100]}...")
    else:
        print("âŒ Aucun modÃ¨le fonctionnel")