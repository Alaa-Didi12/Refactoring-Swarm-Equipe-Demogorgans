# src/utils/orchestrator.py
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
import sys

# Imports des outils du Toolsmith
from src.tools.file_ops import list_files, read_file
from src.tools.analysis import analyze_project, run_static_analysis
from src.tools.testing import run_tests
from src.utils.logger import log_experiment, ActionType

class RefactoringOrchestrator:
    """Orchestrateur principal du Refactoring Swarm."""
    
    def __init__(self, target_dir: str, max_iterations: int = 10, verbose: bool = False):
        """
        Args:
            target_dir: Dossier contenant le code √† refactoriser
            max_iterations: Nombre maximum d'it√©rations de correction
            verbose: Mode verbeux pour le d√©bogage
        """
        self.target_dir = Path(target_dir).resolve()
        self.max_iterations = max_iterations
        self.verbose = verbose
        self.current_iteration = 0
        self.status_history = []
        
        print(f"üéØ Orchestrateur initialis√© pour: {self.target_dir}")
    
    def run(self) -> bool:
        """
        Ex√©cute le processus complet de refactoring.
        
        Returns:
            True si le refactoring a r√©ussi, False sinon
        """
        print("\n" + "="*60)
        print("üîÑ D√âBUT DU PROCESSUS DE REFACTORING")
        print("="*60)
        
        try:
            # √âtape 1: Audit initial
            print("\nüîç √âTAPE 1: AUDIT INITIAL")
            initial_analysis = self._run_audit()
            
            if not initial_analysis.get("success", False):
                print("‚ùå √âchec de l'audit initial")
                return False
            
            # √âtape 2: Boucle de refactoring
            print("\n‚öôÔ∏è  √âTAPE 2: BOUCLE DE REFACTORING")
            success = self._refactoring_loop()
            
            # √âtape 3: Audit final
            print("\nüìã √âTAPE 3: AUDIT FINAL")
            final_analysis = self._run_audit()
            
            # Log de synth√®se
            self._log_summary(initial_analysis, final_analysis)
            
            return success
            
        except Exception as e:
            print(f"‚ùå Erreur dans le processus principal: {e}")
            log_experiment(
                agent_name="Orchestrator",
                model_used="system",
                action=ActionType.DEBUG,
                details={
                    "input_prompt": "Erreur dans run()",
                    "output_response": str(e),
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            return False
    
    def _run_audit(self) -> Dict[str, Any]:
        """
        Ex√©cute l'audit du projet.
        
        Returns:
            R√©sultats de l'analyse
        """
        try:
            print(f"  üìä Analyse du projet: {self.target_dir}")
            
            # Utiliser l'outil d'analyse du Toolsmith
            analysis = analyze_project(str(self.target_dir), "Orchestrator_Audit")
            
            # Calculer le score de qualit√©
            total_issues = analysis.get("summary", {}).get("total_issues", 0)
            total_files = analysis.get("total_files", 0)
            
            print(f"  üìÅ Fichiers analys√©s: {total_files}")
            print(f"  ‚ö†Ô∏è  Probl√®mes d√©tect√©s: {total_issues}")
            
            # Log de l'audit
            log_experiment(
                agent_name="Orchestrator",
                model_used="system",
                action=ActionType.ANALYSIS,
                details={
                    "input_prompt": f"Audit du projet {self.target_dir}",
                    "output_response": f"Analyse compl√®te: {total_files} fichiers, {total_issues} probl√®mes",
                    "analysis_result": analysis,
                    "total_files": total_files,
                    "total_issues": total_issues
                },
                status="SUCCESS"
            )
            
            return {
                "success": True,
                "analysis": analysis,
                "total_files": total_files,
                "total_issues": total_issues,
                "timestamp": time.time()
            }
            
        except Exception as e:
            print(f"  ‚ùå Erreur lors de l'audit: {e}")
            log_experiment(
                agent_name="Orchestrator",
                model_used="system",
                action=ActionType.DEBUG,
                details={
                    "input_prompt": "Erreur dans _run_audit()",
                    "output_response": str(e)
                },
                status="FAILURE"
            )
            return {"success": False, "error": str(e)}
    
    def _refactoring_loop(self) -> bool:
        """
        Ex√©cute la boucle de refactoring avec self-healing.
        
        Returns:
            True si au moins une am√©lioration a √©t√© faite
        """
        print(f"  üîÑ Lancement de la boucle de refactoring (max {self.max_iterations} it√©rations)")
        
        improvements_made = False
        
        for iteration in range(1, self.max_iterations + 1):
            self.current_iteration = iteration
            
            print(f"\n  üîÅ IT√âRATION {iteration}/{self.max_iterations}")
            
            # V√©rifier si les tests passent d√©j√†
            print("  üß™ V√©rification des tests en cours...")
            test_result = run_tests(str(self.target_dir), f"Orchestrator_Iteration_{iteration}")
            
            if test_result.get("success", False):
                print(f"  ‚úÖ Tous les tests passent! (It√©ration {iteration})")
                
                # V√©rifier la qualit√© du code
                print("  üìù V√©rification de la qualit√© du code...")
                code_quality = self._check_code_quality()
                
                if code_quality.get("acceptable", False):
                    print(f"  üéØ Qualit√© du code acceptable - Arr√™t de la boucle")
                    return improvements_made or True
                else:
                    print(f"  ‚ö†Ô∏è  Tests OK mais qualit√© du code insuffisante, continuation...")
            
            # Si les tests √©chouent ou qualit√© insuffisante, lancer la correction
            print("  üõ†Ô∏è  Lancement de la correction...")
            correction_success = self._trigger_fixer(iteration, test_result)
            
            if correction_success:
                improvements_made = True
                print(f"  ‚úÖ Correction appliqu√©e avec succ√®s")
            else:
                print(f"  ‚ö†Ô∏è  Aucune correction appliqu√©e ou erreur")
            
            # Petite pause pour √©viter les boucles trop rapides
            time.sleep(1)
        
        print(f"  ‚è∞ Limite d'it√©rations atteinte ({self.max_iterations})")
        return improvements_made
    
    def _check_code_quality(self) -> Dict[str, Any]:
        """
        V√©rifie la qualit√© globale du code.
        
        Returns:
            Dict avec les r√©sultats de qualit√©
        """
        try:
            # Analyser un fichier repr√©sentatif
            python_files = list_files(str(self.target_dir), ".py", "Orchestrator_Quality_Check")
            
            if not python_files:
                return {"acceptable": True, "reason": "Aucun fichier Python"}
            
            # Prendre le premier fichier comme √©chantillon
            sample_file = python_files[0]
            analysis = run_static_analysis(sample_file, "Orchestrator_Quality_Sample")
            
            issues_count = analysis.get("issues_count", 0)
            acceptable = issues_count < 5  # Seuil arbitraire
            
            return {
                "acceptable": acceptable,
                "sample_file": sample_file,
                "issues_count": issues_count,
                "threshold": 5
            }
            
        except Exception as e:
            print(f"  ‚ùå Erreur v√©rification qualit√©: {e}")
            return {"acceptable": False, "error": str(e)}
    
    def _trigger_fixer(self, iteration: int, test_result: Dict[str, Any]) -> bool:
        """
        D√©clenche le processus de correction.
        
        Args:
            iteration: Num√©ro d'it√©ration
            test_result: R√©sultats des tests
            
        Returns:
            True si une correction a √©t√© appliqu√©e
        """
        try:
            # ICI: Tu devras int√©grer l'agent Fixer quand il sera d√©velopp√©
            # Pour l'instant, on simule une correction
            
            print(f"  ü§ñ [SIMULATION] Appel de l'agent Fixer pour it√©ration {iteration}")
            
            # Log de l'appel au fixer
            log_experiment(
                agent_name="Orchestrator",
                model_used="system",
                action=ActionType.FIX,
                details={
                    "input_prompt": f"D√©clenchement du Fixer - It√©ration {iteration}",
                    "output_response": f"Tests: {'SUCCESS' if test_result.get('success') else 'FAILURE'}",
                    "iteration": iteration,
                    "test_result": test_result,
                    "status": "TRIGGERED"
                },
                status="INFO"
            )
            
            # Simulation: Pour l'instant, retourne True pour continuer
            # √Ä remplacer par l'appel r√©el √† l'agent Fixer
            return True
            
        except Exception as e:
            print(f"  ‚ùå Erreur d√©clenchement fixer: {e}")
            return False
    
    def _log_summary(self, initial: Dict[str, Any], final: Dict[str, Any]) -> None:
        """
        Log un r√©sum√© du processus.
        
        Args:
            initial: Audit initial
            final: Audit final
        """
        print("\n" + "="*60)
        print("üìà R√âSUM√â DU PROCESSUS")
        print("="*60)
        
        initial_issues = initial.get("total_issues", 0)
        final_issues = final.get("total_issues", 0)
        
        print(f"  ‚Ä¢ It√©rations effectu√©es: {self.current_iteration}/{self.max_iterations}")
        print(f"  ‚Ä¢ Probl√®mes initiaux: {initial_issues}")
        print(f"  ‚Ä¢ Probl√®mes finaux: {final_issues}")
        
        if final_issues < initial_issues:
            improvement = ((initial_issues - final_issues) / max(initial_issues, 1)) * 100
            print(f"  üìà Am√©lioration: {improvement:.1f}% de r√©duction des probl√®mes")
        else:
            print(f"  ‚ö†Ô∏è  Aucune am√©lioration d√©tect√©e")
        
        # Log du r√©sum√©
        log_experiment(
            agent_name="Orchestrator",
            model_used="system",
            action=ActionType.ANALYSIS,
            details={
                "input_prompt": "R√©sum√© final du refactoring",
                "output_response": f"Processus termin√© en {self.current_iteration} it√©rations",
                "initial_analysis": initial,
                "final_analysis": final,
                "iterations": self.current_iteration,
                "max_iterations": self.max_iterations,
                "improvement_percentage": (
                    ((initial_issues - final_issues) / max(initial_issues, 1)) * 100
                    if initial_issues > 0 else 0
                )
            },
            status="SUMMARY"
        )